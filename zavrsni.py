# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14ukJK-AlguO2gCITiZTO23kovzxVb4S3
"""

!pip install -q kaggle

from google.colab import files
files.upload()

!mkdir ~/.kaggle
!cp kaggle.json ~/.kaggle/

! chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d toponowicz/spoken-language-identification

import zipfile
zip_ref = zipfile.ZipFile('spoken-language-identification.zip', 'r')
zip_ref.extractall('/content')
zip_ref.close()

import os
!ls /content
train_dir = '/content/train/train'

import os
import librosa
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras import models

from tensorflow.keras.layers import Normalization
from keras.models import Sequential
from tensorflow.keras.utils import Sequence, to_categorical

seed = 42
tf.random.set_seed(seed)
np.random.seed(seed)

def loadPartitionAndLabels(directoryTrain, directoryTest):
    idsTrain = []
    idsTest = []
    labels = dict()

    for filename in os.listdir(directoryTrain):
        idsTrain.append(filename)
        if filename.startswith("en"):
            labels[filename] = 0
        elif filename.startswith("es"):
            labels[filename] = 1
        elif filename.startswith("de"):
            labels[filename] = 2

    for filename in os.listdir(directoryTest):
        idsTest.append(filename)
        if filename.startswith("en"):
            labels[filename] = 0
        elif filename.startswith("es"):
            labels[filename] = 1
        elif filename.startswith("de"):
            labels[filename] = 2

    partition = {"train": idsTrain, "validation": idsTest}

    return partition, labels


class DataGenerator(Sequence):
    'Generates data for Keras'
    def __init__(self, list_IDs, labels, directory, batch_size=32, dim=(32, 32, 1), n_channels=1,
                 n_classes=3, shuffle=True, target_sr=16000, target_size=160000):
        'Initialization'
        self.dim = dim
        self.batch_size = batch_size
        self.labels = labels
        self.list_IDs = list_IDs
        self.directory = directory
        self.n_channels = n_channels
        self.n_classes = n_classes
        self.shuffle = shuffle
        self.target_sr = target_sr
        self.target_size = target_size
        self.on_epoch_end()

    def __len__(self):
        'Denotes the number of batches per epoch'
        return int(np.floor(len(self.list_IDs) / self.batch_size))

    def __getitem__(self, index):
        'Generate one batch of data'
        # Generate indexes of the batch
        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]

        # Find list of IDs
        list_IDs_temp = [self.list_IDs[k] for k in indexes]

        # Generate data
        X, y = self.__data_generation(list_IDs_temp)

        return X, y

    def on_epoch_end(self):
        'Updates indexes after each epoch'
        self.indexes = np.arange(len(self.list_IDs))
        if self.shuffle:
            np.random.shuffle(self.indexes)

    def __data_generation(self, list_IDs_temp):
        'Generates data containing batch_size samples'
        # Initialization
        X = np.empty((self.batch_size, *self.dim))
        y = np.empty((self.batch_size), dtype=int)

        # Generate data
        for i, ID in enumerate(list_IDs_temp):
            path = os.path.join(self.directory, ID)
            audio, _ = librosa.load(path, sr=self.target_sr)
            if len(audio) > self.target_size:
                audio = audio[:self.target_size]
            if len(audio) < self.target_size:
                pad_width = self.target_size - len(audio)
                audio = np.pad(audio, (0, pad_width), mode='constant')

            spectrogram = tf.signal.stft(audio, frame_length=255, frame_step=128)
            spectrogram = tf.abs(spectrogram)
            spectrogram = spectrogram[..., tf.newaxis]

            spectrogram = tf.image.resize(spectrogram, [self.dim[0], self.dim[1]])
            spectrogram = (spectrogram - tf.reduce_mean(spectrogram)) / tf.math.reduce_std(spectrogram)

            # Store sample
            X[i,] = spectrogram.numpy()

            # Store class
            y[i] = self.labels[ID]

        return X, y

directoryTrain = '/content/train/train'
directoryTest = '/content/test/test'

params = {'dim': (32, 32, 1),
          'batch_size': 64}

partition, labels = loadPartitionAndLabels(directoryTrain, directoryTest)

training_generator = DataGenerator(partition['train'], labels, directory=directoryTrain, **params)
validation_generator = DataGenerator(partition['validation'], labels, directory=directoryTest, **params)

X, y = training_generator.__getitem__(0)

input_shape = (32, 32, 1)

model = models.Sequential([
    layers.Input(shape=input_shape),
    layers.Resizing(32, 32),
    Normalization(),
    layers.Conv2D(32, 3, activation='relu'),
    layers.Conv2D(64, 3, activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(128, 3, activation='relu'),
    layers.MaxPooling2D(),
    layers.Dropout(0.25),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(3)  # 3 classes
])


model.compile(
    optimizer=tf.keras.optimizers.Adam(),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=['accuracy'],
)

history = model.fit(
    training_generator,
    validation_data=validation_generator,
    use_multiprocessing=False,
    workers=1,
    epochs=10
)

!kaggle datasets download -d mittalshubham/spoken-languages

import zipfile
zip_ref = zipfile.ZipFile('spoken-languages.zip', 'r')
zip_ref.extractall('/content/testDataset')
zip_ref.close()

eDir = '/content/testDataset/test/en'
enTest = os.listdir(eDir)
print(f"Number of en files: {len(enTest)}")
esDir = '/content/testDataset/test/es'
esTest = os.listdir(esDir)
print(f"Number of es files: {len(esTest)}")
deDir = '/content/testDataset/test/de'
deTest = os.listdir(deDir)
print(f"Number of de files: {len(deTest)}")

import shutil

idsTest = []
labelsTest = {}
for filename in os.listdir(eDir):
    idsTest.append(filename)
    labelsTest[filename] = 0

for filename in os.listdir(esDir):
    idsTest.append(filename)
    labelsTest[filename] = 1

for filename in os.listdir(deDir):
    idsTest.append(filename)
    labelsTest[filename] = 2

class_dirs = [eDir, esDir, deDir]

combined_test_dir = '/content/testDataset/test/combinedDir'

os.makedirs(combined_test_dir, exist_ok=True)

for class_dir in class_dirs:
    for filename in os.listdir(class_dir):
        src_path = os.path.join(class_dir, filename)
        dest_path = os.path.join(combined_test_dir, filename)
        shutil.copy(src_path, dest_path)

test_generator = DataGenerator(idsTest, labelsTest, directory=combined_test_dir, **params)

test_loss, test_accuracy = model.evaluate(test_generator, use_multiprocessing=False, workers=1)
print(f'Test Loss: {test_loss}')
print(f'Test Accuracy: {test_accuracy}')

import os
import numpy as np
import librosa
import tensorflow as tf
from tensorflow.keras import layers, models, Sequential
from tensorflow.keras.layers import Normalization
from tensorflow.keras.utils import Sequence
from tensorflow.keras.optimizers import Adam

seed = 42
tf.random.set_seed(seed)
np.random.seed(seed)

def loadPartitionAndLabels(directoryTrain, directoryTest):
    idsTrain = []
    idsTest = []
    labels = dict()

    for filename in os.listdir(directoryTrain):
        if not filename.endswith('.flac'):
            continue
        idsTrain.append(filename)
        if filename.startswith("en"):
            labels[filename] = 0
        elif filename.startswith("es"):
            labels[filename] = 1
        elif filename.startswith("de"):
            labels[filename] = 2

    for filename in os.listdir(directoryTest):
        if not filename.endswith('.flac'):
            continue
        idsTest.append(filename)
        if filename.startswith("en"):
            labels[filename] = 0
        elif filename.startswith("es"):
            labels[filename] = 1
        elif filename.startswith("de"):
            labels[filename] = 2

    partition = {"train": idsTrain, "validation": idsTest}

    return partition, labels

class DataGenerator(Sequence):
    'Generates data for Keras'
    def __init__(self, list_IDs, labels, directory, batch_size=32, dim=(180, 180, 3), n_channels=3,
                 n_classes=3, shuffle=True, target_sr=16000, target_size=160000):
        'Initialization'
        self.dim = dim
        self.batch_size = batch_size
        self.labels = labels
        self.list_IDs = list_IDs
        self.directory = directory
        self.n_channels = n_channels
        self.n_classes = n_classes
        self.shuffle = shuffle
        self.target_sr = target_sr
        self.target_size = target_size
        self.on_epoch_end()

    def __len__(self):
        'Denotes the number of batches per epoch'
        return int(np.floor(len(self.list_IDs) / self.batch_size))

    def __getitem__(self, index):
        'Generate one batch of data'
        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]

        list_IDs_temp = [self.list_IDs[k] for k in indexes]

        X, y = self.__data_generation(list_IDs_temp)

        return X, y

    def on_epoch_end(self):
        'Updates indexes after each epoch'
        self.indexes = np.arange(len(self.list_IDs))
        if self.shuffle:
            np.random.shuffle(self.indexes)

    def __data_generation(self, list_IDs_temp):
        'Generates data containing batch_size samples'
        # Initialization
        X = np.empty((self.batch_size, *self.dim))
        y = np.empty((self.batch_size), dtype=int)

        # Generate data
        for i, ID in enumerate(list_IDs_temp):
            path = os.path.join(self.directory, ID)
            audio, _ = librosa.load(path, sr=self.target_sr)
            if len(audio) > self.target_size:
                audio = audio[:self.target_size]
            if len(audio) < self.target_size:
                pad_width = self.target_size - len(audio)
                audio = np.pad(audio, (0, pad_width), mode='constant')

            spectrogram = tf.signal.stft(audio, frame_length=255, frame_step=128)
            spectrogram = tf.abs(spectrogram)

            spectrogram = spectrogram[..., tf.newaxis]

            spectrogram = tf.image.resize(spectrogram, self.dim[:2])

            spectrogram = tf.image.grayscale_to_rgb(spectrogram)

            spectrogram = (spectrogram - tf.reduce_mean(spectrogram)) / tf.math.reduce_std(spectrogram)

            X[i,] = spectrogram.numpy()

            y[i] = self.labels[ID]

        return X, y


directoryTrain = '/content/train/train'
directoryTest = '/content/test/test'

params = {'dim': (180, 180, 3),
          'batch_size': 64}

partition, labels = loadPartitionAndLabels(directoryTrain, directoryTest)

training_generator = DataGenerator(partition['train'], labels, directory=directoryTrain, **params)
validation_generator = DataGenerator(partition['validation'], labels, directory=directoryTest, **params)

X, y = training_generator.__getitem__(0)

input_shape = (180, 180, 3)

imported_model = tf.keras.applications.ResNet50(include_top=False, input_shape=input_shape, pooling='avg', weights='imagenet')

for layer in imported_model.layers:
    layer.trainable = False

model = Sequential()
model.add(imported_model)
model.add(layers.Flatten())
model.add(layers.Dense(512, activation='relu'))
model.add(layers.BatchNormalization())
model.add(layers.Dropout(0.5))
model.add(layers.Dense(256, activation='relu'))
model.add(layers.BatchNormalization())
model.add(layers.Dropout(0.5))
model.add(layers.Dense(3, activation='softmax'))  # 3 classes
model.summary()

model.compile(
    optimizer=Adam(lr=0.001),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy'],
)

history = model.fit(
    training_generator,
    validation_data=validation_generator,
    epochs=10
)

!kaggle datasets download -d mittalshubham/spoken-languages
import shutil
import zipfile
import os
zip_ref = zipfile.ZipFile('spoken-languages.zip', 'r')
zip_ref.extractall('/content/testDataset')
zip_ref.close()
eDir = '/content/testDataset/test/en'
enTest = os.listdir(eDir)
print(f"Number of en files: {len(enTest)}")
esDir = '/content/testDataset/test/es'
esTest = os.listdir(esDir)
print(f"Number of es files: {len(esTest)}")
deDir = '/content/testDataset/test/de'
deTest = os.listdir(deDir)
print(f"Number of de files: {len(deTest)}")

import os
import shutil

idsTest = []
labelsTest = {}
for filename in os.listdir(eDir):
    idsTest.append(filename)
    labelsTest[filename] = 0

for filename in os.listdir(esDir):
    idsTest.append(filename)
    labelsTest[filename] = 1

for filename in os.listdir(deDir):
    idsTest.append(filename)
    labelsTest[filename] = 2

class_dirs = [eDir, esDir, deDir]

combined_test_dir = '/content/testDataset/test/combinedDir'

os.makedirs(combined_test_dir, exist_ok=True)

for class_dir in class_dirs:
    for filename in os.listdir(class_dir):
        src_path = os.path.join(class_dir, filename)
        dest_path = os.path.join(combined_test_dir, filename)
        shutil.copy(src_path, dest_path)

test_generator = DataGenerator(idsTest, labelsTest, directory=combined_test_dir, **params)

test_loss, test_accuracy = model.evaluate(test_generator, use_multiprocessing=False, workers=1)
print(f'Test Loss: {test_loss}')
print(f'Test Accuracy: {test_accuracy}')

import librosa
import os
import numpy as np

directory = '/content/train/train'
sample_file = next((f for f in os.listdir(directory) if f.endswith('.flac')), None)
if sample_file:
    sample_audio, _ = librosa.load(os.path.join(directory, sample_file), sr=16000)
    sample_mfccs = librosa.feature.mfcc(y=sample_audio, sr=16000, n_mfcc=13, n_fft=4096, hop_length=512)
    print("Sample MFCC shape:", sample_mfccs.shape)

import os
import librosa
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models, Sequential
from tensorflow.keras.utils import Sequence
from tensorflow.keras.optimizers import Adam


seed = 42
tf.random.set_seed(seed)
np.random.seed(seed)
def loadPartitionAndLabels(directoryTrain, directoryTest):
    idsTrain = []
    idsTest = []
    labels = dict()

    # Load training data
    for filename in os.listdir(directoryTrain):
        if not filename.endswith('.flac'):
            continue
        idsTrain.append(filename)
        if filename.startswith("en"):
            labels[filename] = 0
        elif filename.startswith("es"):
            labels[filename] = 1
        elif filename.startswith("de"):
            labels[filename] = 2

    # Load test data
    for filename in os.listdir(directoryTest):
        if not filename.endswith('.flac'):
            continue
        idsTest.append(filename)
        if filename.startswith("en"):
            labels[filename] = 0
        elif filename.startswith("es"):
            labels[filename] = 1
        elif filename.startswith("de"):
            labels[filename] = 2

    partition = {"train": idsTrain, "validation": idsTest}

    # Debug prints
    print("Training IDs:", idsTrain[:5])  # Print the first 5 training IDs
    print("Validation IDs:", idsTest[:5])  # Print the first 5 validation IDs
    print("Labels:", {k: labels[k] for k in list(labels)[:5]})  # Print the first 5 labels

    return partition, labels

class DataGenerator(Sequence):
    'Generates data for Keras'
    def __init__(self, list_IDs, labels, directory,batch_size=32, dim=(13, 313), n_channels=1,
                 n_classes=3, shuffle=True, target_sr=16000, target_size=160000):
        'Initialization'
        self.dim = dim  # (n_mfcc, time_steps)
        self.batch_size = batch_size
        self.labels = labels
        self.list_IDs = list_IDs
        self.directory = directory
        self.n_channels = n_channels
        self.n_classes = n_classes
        self.shuffle = shuffle
        self.target_sr = target_sr
        self.target_size = target_size
        self.on_epoch_end()

    def __len__(self):
        'Denotes the number of batches per epoch'
        return int(np.floor(len(self.list_IDs) / self.batch_size))

    def __getitem__(self, index):
        'Generate one batch of data'
        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]
        list_IDs_temp = [self.list_IDs[k] for k in indexes]
        X, y = self.__data_generation(list_IDs_temp)
        return X, y

    def on_epoch_end(self):
        'Updates indexes after each epoch'
        self.indexes = np.arange(len(self.list_IDs))
        if self.shuffle:
            np.random.shuffle(self.indexes)

    def __data_generation(self, list_IDs_temp):
        'Generates data containing batch_size samples'
        X = np.empty((self.batch_size, *self.dim, self.n_channels))
        y = np.empty((self.batch_size), dtype=int)

        # Generate data
        for i, ID in enumerate(list_IDs_temp):
            path = os.path.join(self.directory, ID)
            audio, _ = librosa.load(path, sr=self.target_sr)
            if len(audio) > self.target_size:
                audio = audio[:self.target_size]
            if len(audio) < self.target_size:
                pad_width = self.target_size - len(audio)
                audio = np.pad(audio, (0, pad_width), mode='constant')

            # Compute the MFCC
            mfcc = librosa.feature.mfcc(y=audio, sr=self.target_sr, n_mfcc=self.dim[0], n_fft=4096, hop_length=512)

            # Ensure the MFCC has the right shape
            if mfcc.shape[1] > self.dim[1]:
                mfcc = mfcc[:, :self.dim[1]]
            elif mfcc.shape[1] < self.dim[1]:
                pad_width = self.dim[1] - mfcc.shape[1]
                mfcc = np.pad(mfcc, ((0, 0), (0, pad_width)), mode='constant')

            mfcc = (mfcc - np.mean(mfcc))/ np.std(mfcc)
            # Add an extra dimension for the channel
            mfcc = np.expand_dims(mfcc, axis=-1)

            # Store sample
            X[i,] = mfcc

            # Store class
            y[i] = self.labels[ID]

        return X, y

# Example usage:
directoryTrain = '/content/train/train'
directoryTest = '/content/test/test'


mfcc_shape = (13, 313)  # Example shape, adjust based on your data

params = {
    'dim': mfcc_shape,
    'batch_size': 64,
    'n_channels': 1,
    'n_classes': 3,
    'shuffle': True
}




partition, labels = loadPartitionAndLabels(directoryTrain, directoryTest)

training_generator = DataGenerator(partition['train'], labels, directoryTrain, **params)
validation_generator = DataGenerator(partition['validation'], labels, directoryTest, **params)

input_shape = (*mfcc_shape, 1)

sample_shape = (13, 313, 1)

# Model definition
# Build model
# Based on: https://www.geeksforgeeks.org/python-image-classification-using-keras/
model = models.Sequential()
model.add(layers.Conv2D(32,
                        (2, 2),
                        activation='relu',
                        input_shape=sample_shape))
model.add(layers.MaxPooling2D(pool_size=(2, 2)))

model.add(layers.Conv2D(32, (2, 2), activation='relu'))
model.add(layers.MaxPooling2D(pool_size=(2, 2)))

# model.add(layers.Conv2D(64, (2, 2), activation='relu'))
# model.add(layers.MaxPooling2D(pool_size=(2, 2)))

# Classifier
model.add(layers.Flatten())
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dropout(0.5))
model.add(layers.Dense(3, activation='softmax'))

model.summary()

model.compile(
    optimizer=Adam(lr=0.001),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy'],
)

history = model.fit(
    training_generator,
    validation_data=validation_generator,
    epochs=5
)

# model.compile(loss='binary_crossentropy',
#               optimizer='rmsprop',
#               metrics=['acc'])
# # Train
# history = model.fit(training_generator,
#                     epochs=10,
#                     batch_size=64,
#                     validation_data=validation_generator)

import os
import shutil

idsTest = []
labelsTest = {}
for filename in os.listdir(eDir):
    idsTest.append(filename)
    labelsTest[filename] = 0

for filename in os.listdir(esDir):
    idsTest.append(filename)
    labelsTest[filename] = 1

for filename in os.listdir(deDir):
    idsTest.append(filename)
    labelsTest[filename] = 2

class_dirs = [eDir, esDir, deDir]

combined_test_dir = '/content/testDataset/test/combinedDir'

os.makedirs(combined_test_dir, exist_ok=True)

for class_dir in class_dirs:
    for filename in os.listdir(class_dir):
        src_path = os.path.join(class_dir, filename)
        dest_path = os.path.join(combined_test_dir, filename)
        shutil.copy(src_path, dest_path)

test_generator = DataGenerator(idsTest, labelsTest, directory=combined_test_dir, **params)

test_loss, test_accuracy = model.evaluate(test_generator, use_multiprocessing=False, workers=1)
print(f'Test Loss: {test_loss}')
print(f'Test Accuracy: {test_accuracy}')